{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "LR = 1e-3\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "goal_steps = 500\n",
    "score_requirement = 65\n",
    "initial_games = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_random_games_first():\n",
    "    # Each of these is its own game.\n",
    "    for episode in range(10):\n",
    "        env.reset()\n",
    "        # this is each frame, up to 200...but we wont make it that far.\n",
    "        for t in range(20):\n",
    "            # This will display the environment\n",
    "            # Only display if you really want to see it.\n",
    "            # Takes much longer to display it.\n",
    "            env.render()\n",
    "            \n",
    "            # This will just create a sample action in any environment.\n",
    "            # In this environment, the action can be 0 or 1, which is left or right\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "            # this executes the environment with an action, \n",
    "            # and returns the observation of the environment, \n",
    "            # the reward, if the env is over, and other info.\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "    env.close()\n",
    "                \n",
    "some_random_games_first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_population():\n",
    "    # x for \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    # all scores:\n",
    "    scores = []\n",
    "    # just the scores that met our threshold:\n",
    "    accepted_scores = []\n",
    "    # iterate through however many games we want:\n",
    "    for _ in range(initial_games):\n",
    "        score = 0\n",
    "        \n",
    "        # moves specifically from this environment:\n",
    "        game_memory = []\n",
    "        # previous observation that we saw\n",
    "        prev_observation = []\n",
    "        # for each frame in 200\n",
    "        for _ in range(goal_steps):\n",
    "            # choose random action (0 or 1)\n",
    "            action = random.randrange(0,2)\n",
    "            # do it!\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            # notice that the observation is returned FROM the action\n",
    "            # so we'll store the previous observation here, pairing\n",
    "            # the prev observation to the action we'll take.\n",
    "            if len(prev_observation) > 0 :\n",
    "                game_memory.append([prev_observation, action])\n",
    "            prev_observation = observation\n",
    "            score+=reward\n",
    "            if done:\n",
    "                env.reset()\n",
    "                break\n",
    "\n",
    "        # IF our score is higher than our threshold, we'd like to save\n",
    "        # every move we made\n",
    "        # NOTE the reinforcement methodology here. \n",
    "        # all we're doing is reinforcing the score, we're not trying \n",
    "        # to influence the machine in any way as to HOW that score is \n",
    "        # reached.\n",
    "        if score >= score_requirement:\n",
    "            accepted_scores.append(score)\n",
    "            for data in game_memory:\n",
    "                # convert to one-hot (this is the output layer for our neural network)\n",
    "                if data[1] == 1:\n",
    "                    output = [0,1]\n",
    "                elif data[1] == 0:\n",
    "                    output = [1,0]\n",
    "                    \n",
    "                # saving our training data\n",
    "                x_train.append(data[0])\n",
    "                y_train.append(output)\n",
    "\n",
    "        # reset env to play again\n",
    "        \n",
    "        # save overall scores\n",
    "        scores.append(score)\n",
    "    \n",
    "    # just in case you wanted to reference later\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    np.save('saved x_train.npy',x_train)\n",
    "    np.save('saved y_train.npy',y_train)\n",
    "    \n",
    "    # some stats here, to further illustrate the neural network magic!\n",
    "    print('Average accepted score:',np.mean(accepted_scores))\n",
    "    print('Median score for accepted scores:',np.median(accepted_scores))\n",
    "    print(len(accepted_scores))\n",
    "    \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accepted score: 76.02102102102103\n",
      "Median score for accepted scores: 73.0\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = initial_population()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74946, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               640       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 97,058\n",
      "Trainable params: 95,330\n",
      "Non-trainable params: 1,728\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_NN():\n",
    "    m = Sequential()\n",
    "    \n",
    "    m.add(Dense(128, input_dim=4))\n",
    "    m.add(Activation('relu'))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Dropout(0.2))\n",
    "    \n",
    "    m.add(Dense(128))\n",
    "    m.add(Activation('relu'))\n",
    "    m.add(BatchNormalization())\n",
    "    \n",
    "    m.add(Dense(128))\n",
    "    m.add(Activation('relu'))\n",
    "    m.add(BatchNormalization())\n",
    "    \n",
    "    m.add(Dense(128))\n",
    "    m.add(Activation('relu'))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Dropout(0.2))\n",
    "    \n",
    "    m.add(Dense(128))\n",
    "    m.add(Activation('relu'))\n",
    "    m.add(BatchNormalization())\n",
    "    \n",
    "    m.add(Dense(128))\n",
    "    m.add(Activation('relu'))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Dropout(0.2))\n",
    "    \n",
    "    m.add(Dense(64))\n",
    "    m.add(Activation('relu'))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Dropout(0.2))\n",
    "    \n",
    "    m.add(Dense(32))\n",
    "    m.add(Activation('relu'))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Dropout(0.2))\n",
    "    \n",
    "    m.add(Dense(2))\n",
    "    m.add(Activation('softmax'))\n",
    "    \n",
    "    m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    m.summary()\n",
    "    return m\n",
    "m = build_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 67451 samples, validate on 7495 samples\n",
      "Epoch 1/30\n",
      "67451/67451 [==============================] - 19s 278us/step - loss: 0.6794 - acc: 0.5917 - val_loss: 0.6551 - val_acc: 0.6041\n",
      "Epoch 2/30\n",
      "67451/67451 [==============================] - 15s 219us/step - loss: 0.6619 - acc: 0.6062 - val_loss: 0.6559 - val_acc: 0.6025\n",
      "Epoch 3/30\n",
      "67451/67451 [==============================] - 15s 219us/step - loss: 0.6595 - acc: 0.6049 - val_loss: 0.6549 - val_acc: 0.6049\n",
      "Epoch 4/30\n",
      "67451/67451 [==============================] - 15s 219us/step - loss: 0.6579 - acc: 0.6070 - val_loss: 0.6511 - val_acc: 0.6075\n",
      "Epoch 5/30\n",
      "67451/67451 [==============================] - 15s 219us/step - loss: 0.6570 - acc: 0.6067 - val_loss: 0.6513 - val_acc: 0.6025\n",
      "Epoch 6/30\n",
      "67451/67451 [==============================] - 15s 221us/step - loss: 0.6565 - acc: 0.6090 - val_loss: 0.6508 - val_acc: 0.6143\n",
      "Epoch 7/30\n",
      "67451/67451 [==============================] - 15s 222us/step - loss: 0.6557 - acc: 0.6086 - val_loss: 0.6505 - val_acc: 0.6089\n",
      "Epoch 8/30\n",
      "67451/67451 [==============================] - 15s 223us/step - loss: 0.6556 - acc: 0.6076 - val_loss: 0.6508 - val_acc: 0.6077\n",
      "Epoch 9/30\n",
      "67451/67451 [==============================] - 15s 222us/step - loss: 0.6560 - acc: 0.6092 - val_loss: 0.6532 - val_acc: 0.6060\n",
      "Epoch 10/30\n",
      "67451/67451 [==============================] - 15s 223us/step - loss: 0.6554 - acc: 0.6103 - val_loss: 0.6515 - val_acc: 0.6063\n",
      "Epoch 11/30\n",
      "67451/67451 [==============================] - 15s 222us/step - loss: 0.6546 - acc: 0.6101 - val_loss: 0.6512 - val_acc: 0.6091\n",
      "Epoch 12/30\n",
      "67451/67451 [==============================] - 15s 224us/step - loss: 0.6543 - acc: 0.6120 - val_loss: 0.6511 - val_acc: 0.6079\n",
      "Epoch 13/30\n",
      "67451/67451 [==============================] - 15s 226us/step - loss: 0.6538 - acc: 0.6107 - val_loss: 0.6531 - val_acc: 0.6120\n",
      "Epoch 14/30\n",
      "67451/67451 [==============================] - 15s 229us/step - loss: 0.6538 - acc: 0.6120 - val_loss: 0.6512 - val_acc: 0.6083\n",
      "Epoch 15/30\n",
      "67451/67451 [==============================] - 15s 229us/step - loss: 0.6541 - acc: 0.6083 - val_loss: 0.6505 - val_acc: 0.6029\n",
      "Epoch 16/30\n",
      "67451/67451 [==============================] - 15s 229us/step - loss: 0.6544 - acc: 0.6117 - val_loss: 0.6502 - val_acc: 0.6123\n",
      "Epoch 17/30\n",
      "67451/67451 [==============================] - 16s 230us/step - loss: 0.6541 - acc: 0.6111 - val_loss: 0.6493 - val_acc: 0.6083\n",
      "Epoch 18/30\n",
      "67451/67451 [==============================] - 15s 230us/step - loss: 0.6545 - acc: 0.6093 - val_loss: 0.6495 - val_acc: 0.6084\n",
      "Epoch 19/30\n",
      "67451/67451 [==============================] - 15s 230us/step - loss: 0.6533 - acc: 0.6103 - val_loss: 0.6507 - val_acc: 0.6105\n",
      "Epoch 20/30\n",
      "67451/67451 [==============================] - 16s 230us/step - loss: 0.6540 - acc: 0.6099 - val_loss: 0.6500 - val_acc: 0.6112\n",
      "Epoch 21/30\n",
      "67451/67451 [==============================] - 15s 229us/step - loss: 0.6537 - acc: 0.6095 - val_loss: 0.6497 - val_acc: 0.6077\n",
      "Epoch 22/30\n",
      "67451/67451 [==============================] - 16s 230us/step - loss: 0.6533 - acc: 0.6085 - val_loss: 0.6505 - val_acc: 0.6085\n",
      "Epoch 23/30\n",
      "67451/67451 [==============================] - 15s 230us/step - loss: 0.6534 - acc: 0.6093 - val_loss: 0.6485 - val_acc: 0.6097\n",
      "Epoch 24/30\n",
      "67451/67451 [==============================] - 16s 231us/step - loss: 0.6531 - acc: 0.6117 - val_loss: 0.6502 - val_acc: 0.6075\n",
      "Epoch 25/30\n",
      "67451/67451 [==============================] - 16s 232us/step - loss: 0.6530 - acc: 0.6114 - val_loss: 0.6489 - val_acc: 0.6080\n",
      "Epoch 26/30\n",
      "67451/67451 [==============================] - 16s 232us/step - loss: 0.6534 - acc: 0.6082 - val_loss: 0.6512 - val_acc: 0.6087\n",
      "Epoch 27/30\n",
      "67451/67451 [==============================] - 16s 231us/step - loss: 0.6535 - acc: 0.6097 - val_loss: 0.6532 - val_acc: 0.6085\n",
      "Epoch 28/30\n",
      "67451/67451 [==============================] - 16s 232us/step - loss: 0.6535 - acc: 0.6082 - val_loss: 0.6498 - val_acc: 0.6041\n",
      "Epoch 29/30\n",
      "67451/67451 [==============================] - 15s 229us/step - loss: 0.6529 - acc: 0.6113 - val_loss: 0.6516 - val_acc: 0.6105\n",
      "Epoch 30/30\n",
      "67451/67451 [==============================] - 16s 230us/step - loss: 0.6531 - acc: 0.6111 - val_loss: 0.6499 - val_acc: 0.6039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x157ff38d400>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(x_train, y_train, epochs=30, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current game: 50 satisfied games: 50\n",
      "Average Score: 161.04\n",
      "highest: 200.0\n",
      "lowest: 99.0\n",
      "choice 1:0.5091902632886239  choice 0:0.49080973671137607\n",
      "current game: 100 satisfied games: 100\n",
      "Average Score: 158.57\n",
      "highest: 200.0\n",
      "lowest: 95.0\n",
      "choice 1:0.5097433310210002  choice 0:0.49025666897899983\n",
      "current game: 150 satisfied games: 150\n",
      "Average Score: 162.52666666666667\n",
      "highest: 200.0\n",
      "lowest: 95.0\n",
      "choice 1:0.5092907830509865  choice 0:0.4907092169490135\n",
      "current game: 200 satisfied games: 199\n",
      "Average Score: 164.3718592964824\n",
      "highest: 200.0\n",
      "lowest: 95.0\n",
      "choice 1:0.5089446256057051  choice 0:0.4910553743942949\n",
      "current game: 250 satisfied games: 246\n",
      "Average Score: 166.890243902439\n",
      "highest: 200.0\n",
      "lowest: 95.0\n",
      "choice 1:0.5084995057025052  choice 0:0.49150049429749476\n",
      "current game: 300 satisfied games: 287\n",
      "Average Score: 167.14982578397212\n",
      "highest: 200.0\n",
      "lowest: 95.0\n",
      "choice 1:0.5089560706759605  choice 0:0.49104392932403956\n",
      "current game: 350 satisfied games: 330\n",
      "Average Score: 168.66363636363636\n",
      "highest: 200.0\n",
      "lowest: 95.0\n",
      "choice 1:0.5086311274085862  choice 0:0.4913688725914138\n",
      "current game: 400 satisfied games: 371\n",
      "Average Score: 169.9757412398922\n",
      "highest: 200.0\n",
      "lowest: 95.0\n",
      "choice 1:0.5086554279418645  choice 0:0.4913445720581355\n",
      "current game: 450 satisfied games: 410\n",
      "Average Score: 171.34878048780487\n",
      "highest: 200.0\n",
      "lowest: 95.0\n",
      "choice 1:0.5084793366323391  choice 0:0.4915206633676608\n",
      "current game: 500 satisfied games: 443\n",
      "Average Score: 172.06546275395033\n",
      "highest: 200.0\n",
      "lowest: 95.0\n",
      "choice 1:0.5086989044930263  choice 0:0.4913010955069736\n",
      "current game: 550 satisfied games: 480\n",
      "Average Score: 173.14583333333334\n",
      "highest: 200.0\n",
      "lowest: 95.0\n",
      "choice 1:0.508659074250331  choice 0:0.4913409257496691\n",
      "current game: 600 satisfied games: 515\n",
      "Average Score: 173.96893203883496\n",
      "highest: 200.0\n",
      "lowest: 95.0\n",
      "choice 1:0.5087053593776594  choice 0:0.4912946406223406\n",
      "current game: 650 satisfied games: 541\n",
      "Average Score: 174.75970425138632\n",
      "highest: 200.0\n",
      "lowest: 95.0\n",
      "choice 1:0.5088113465734679  choice 0:0.49118865342653206\n",
      "current game: 700 satisfied games: 564\n",
      "Average Score: 175.45744680851064\n",
      "highest: 200.0\n",
      "lowest: 95.0\n",
      "choice 1:0.5088433621096937  choice 0:0.4911566378903062\n",
      "current game: 750 satisfied games: 594\n",
      "Average Score: 176.4124579124579\n",
      "highest: 200.0\n",
      "lowest: 95.0\n",
      "choice 1:0.5087278804404322  choice 0:0.4912721195595679\n",
      "current game: 800 satisfied games: 618\n",
      "Average Score: 177.2168284789644\n",
      "highest: 200.0\n",
      "lowest: 95.0\n",
      "choice 1:0.5086700008984457  choice 0:0.49132999910155434\n",
      "current game: 850 satisfied games: 627\n",
      "Average Score: 177.5103668261563\n",
      "highest: 200.0\n",
      "lowest: 95.0\n",
      "choice 1:0.5089432274862739  choice 0:0.49105677251372615\n",
      "current game: 900 satisfied games: 645\n",
      "Average Score: 178.12713178294572\n",
      "highest: 200.0\n",
      "lowest: 95.0\n",
      "choice 1:0.508921520071747  choice 0:0.49107847992825304\n",
      "current game: 950 satisfied games: 645\n",
      "Average Score: 178.12713178294572\n",
      "highest: 200.0\n",
      "lowest: 95.0\n",
      "choice 1:0.5089197287894563  choice 0:0.4910802712105437\n",
      "Average Score: 178.12713178294572\n",
      "highest: 200.0\n",
      "lowest: 95.0\n",
      "choice 1:0.5090070763410905  choice 0:0.4909929236589095\n",
      "160\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "choices = []\n",
    "x_train = []\n",
    "y_train = []\n",
    "for each_game in range(1000):\n",
    "    if each_game % 50 ==0 and len(scores)!=0:\n",
    "        print('current game:',each_game, 'satisfied games:', len(scores))\n",
    "        print('Average Score:',sum(scores)/len(scores))\n",
    "        print('highest:', max(scores))\n",
    "        print('lowest:', min(scores))\n",
    "        print('choice 1:{}  choice 0:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices)))\n",
    "        score_requirement += 5\n",
    "    score = 0\n",
    "    game_memory = []\n",
    "    prev_obs = []\n",
    "    env.reset()\n",
    "    for _ in range(goal_steps):\n",
    "        if each_game % 40 == 0:\n",
    "            env.render()\n",
    "\n",
    "        if len(prev_obs)==0:\n",
    "            action = random.randrange(0,2)\n",
    "        else:\n",
    "            action = np.argmax(m.predict(prev_obs.reshape(1,4),batch_size=1 ))\n",
    "\n",
    "        choices.append(action)\n",
    "                \n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        prev_obs = new_observation\n",
    "        game_memory.append([new_observation, action])\n",
    "        score+=reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    if score >= score_requirement * 1.3:\n",
    "        for data in game_memory:\n",
    "            # convert to one-hot (this is the output layer for our neural network)\n",
    "            if data[1] == 1:\n",
    "                output = [0,1]\n",
    "            elif data[1] == 0:\n",
    "                output = [1,0]\n",
    "\n",
    "            # saving our training data\n",
    "            x_train.append(data[0])\n",
    "            y_train.append(output)\n",
    "\n",
    "        # save overall scores\n",
    "        scores.append(score)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "np.save('saved x_train.npy',x_train)\n",
    "np.save('saved y_train.npy',y_train)\n",
    "    \n",
    "print('Average Score:',sum(scores)/len(scores))\n",
    "print('highest:', max(scores))\n",
    "print('lowest:', min(scores))\n",
    "print('choice 1:{}  choice 0:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices)))\n",
    "print(score_requirement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
