{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-learning Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen = 2000)\n",
    "        self.gamma = 0.96    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.993\n",
    "        self.model = self._build_model()\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(200, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(self.action_size, activation='tanh'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam())\n",
    "        return model\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return (np.random.rand(4) * 2) -1\n",
    "        act_values = self.model.predict(state)\n",
    "        act_values = [item for sublist in act_values for item in sublist]\n",
    "        return act_values # returns action\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                  target = reward + self.gamma * self.model.predict(next_state)[0]\n",
    "            target_f = self.model.predict(state)\n",
    "            \n",
    "#             print(\"target_f\", target_f, target)\n",
    "            target_f[:] = target\n",
    "#             print(\"target_f\", target_f, target)\n",
    "            self.model.fit(state, target_f, epochs= 1, verbose=0 )\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    def save_model(self):\n",
    "        self.model.save('./checkpoint.h5')\n",
    "    def load_model(self):\n",
    "        self.model.load_weights('./checkpoint.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_consec_300(x):\n",
    "    cnt = 0\n",
    "    for i in range(len(x)):\n",
    "        if x[i] >= 300:\n",
    "            cnt += 1\n",
    "        else:\n",
    "            cnt = 0\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, \n",
      "score: 181, count 300:0 max:0 min:0 mean:0.0\n",
      "0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, \n",
      "score: 204, count 300:0 max:181 min:0 mean:90.5\n",
      "0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, \n",
      "score: 173, count 300:0 max:204 min:0 mean:128.33333333333334\n",
      "0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, \n",
      "score: 116, count 300:0 max:204 min:0 mean:139.5\n",
      "0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, \n",
      "score: 77, count 300:0 max:204 min:0 mean:134.8\n",
      "0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300, 305, 310, 315, 320, 325, 330, 335, 340, 345, 350, 355, 360, 365, 370, 375, 380, 385, 390, 395, 400, 405, 410, 415, 420, 425, 430, 435, 440, 445, \n",
      "score: 449, count 300:0 max:204 min:0 mean:125.16666666666667\n",
      "0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, \n",
      "score: 65, count 300:1 max:449 min:0 mean:171.42857142857142\n",
      "0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, \n",
      "score: 78, count 300:0 max:449 min:0 mean:158.125\n",
      "0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, \n",
      "score: 145, count 300:0 max:449 min:0 mean:149.22222222222223\n",
      "0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, \n",
      "score: 85, count 300:0 max:449 min:0 mean:148.8\n",
      "0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, \n",
      "score: 63, count 300:0 max:449 min:0 mean:143.0\n",
      "0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, \n",
      "score: 73, count 300:0 max:449 min:0 mean:136.33333333333334\n",
      "0, 5, 10, 15, 20, 25, 30, 35, "
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make('BipedalWalker-v2')\n",
    "state_size = 24\n",
    "action_size = 4\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "# agent.load_model()\n",
    "done = False\n",
    "batch_size = 32\n",
    "game_history = [0]\n",
    "t_steps = 0\n",
    "for e in range(2000):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(450):\n",
    "        # env.render()\n",
    "        if time % 5 == 0:\n",
    "            print(time, end=', ')\n",
    "        \n",
    "\n",
    "\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward if not done else -100\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        \n",
    "        if len(agent.memory) > batch_size +5:\n",
    "\n",
    "            agent.replay(batch_size + int(time / 10))\n",
    "    print(\"\\nscore: {}, count 300:{} max:{} min:{} mean:{}\".format(time ,count_consec_300(game_history), max(game_history), min(game_history), np.mean(game_history)))\n",
    "    game_history.append(time)\n",
    "    done = False\n",
    "    if e % 5 == 4:\n",
    "        agent.save_model()\n",
    "        batch_size += 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 446)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.plot(game_history)\n",
    "len(game_history), len(agent.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(game_history)\n",
    "np.save('checkpoint350_gameHist',game_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0512579599480434"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "1.0005 ** 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
