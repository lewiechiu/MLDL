{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "# from  lunarLanding import DQNAgent\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Activation, Input, concatenate, add\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())\n",
    "\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "import itertools\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from scipy.special import softmax\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "import time\n",
    "\n",
    "import os\n",
    "\n",
    "# Deep Q-learning Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, memsize = 7000, ga = 0.95, explore_rate = 1, explore_decay = 0.995):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen = memsize)\n",
    "        self.gamma = ga    # discount rate\n",
    "        self.epsilon = explore_rate  # exploration rate\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = explore_decay\n",
    "        self.target_model = Sequential()\n",
    "        self.engine_model = Sequential()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        if not done:\n",
    "            self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state,action):\n",
    "                \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            chose = np.random.randint(0,4)\n",
    "            return chose\n",
    "        \n",
    "        act_values = 0\n",
    "        for i in range(4):\n",
    "            if i\n",
    "        act_values = self.target_model.predict([state,action])\n",
    "        chose = np.argmax(act_values)\n",
    "        if chose == 0:\n",
    "            return chose\n",
    "        if chose == 1:\n",
    "            return chose\n",
    "        if chose == 2:\n",
    "            return chose\n",
    "        if chose == 3:\n",
    "            return chose\n",
    "        return act_values\n",
    "#     def replay(self, batch_size):\n",
    "#         minibatch = random.sample(self.memory, batch_size)\n",
    "#         xs = []\n",
    "#         ys = []\n",
    "\n",
    "#         for state, action, reward, next_state, done in minibatch:\n",
    "#             target = reward\n",
    "\n",
    "#             if not done:\n",
    "#                 target = reward + np.multiply (self.gamma , self.model.predict(next_state)[0] )\n",
    "#             else:\n",
    "#                 target = np.multiply (self.gamma , self.model.predict(next_state)[0] )\n",
    "                \n",
    "#             xs.append(state[0])\n",
    "#             ys.append(target)\n",
    "#         xs = np.array(xs)\n",
    "#         ys = np.array(ys)\n",
    "#         self.model.fit(xs, ys, epochs= 1, verbose=0 , batch_size=batch_size)\n",
    "                \n",
    "#         if self.epsilon > self.epsilon_min:\n",
    "#             self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def save_model(self, model_name = './checkpoint.h5', mem_name = 'memory'):\n",
    "        self.target_model.save(model_name)\n",
    "\n",
    "    def load_model(self,  model_name = './checkpoint.h5' , mem_name = 'memory.npy'):\n",
    "        self.target_model.load_weights(model_name)\n",
    "        self.engine_model.load_weights(model_name)\n",
    "        self.memory = np.load(mem_name, allow_pickle=True)\n",
    "        self.memory = deque(self.memory)\n",
    "    def learn (self):\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay** 10\n",
    "        output = list(itertools.islice(agent.memory, 0, None))\n",
    "        xs = []\n",
    "        ys = []\n",
    "        for i in output:\n",
    "            ys.append(i[2])\n",
    "            print('reward',i[2])\n",
    "            xs.append([i[0][0] , i[1][0]])\n",
    "        xs = np.array(xs)\n",
    "        ys = np.array(ys)\n",
    "        self.engine_model.fit(xs, ys, epochs = 2 , verbose = 0)\n",
    "        del self.memory\n",
    "        \n",
    "        self.memory = deque(maxlen = 9000)\n",
    "#         print(len(self.memory))\n",
    "        self.engine_model.save_weights(\"checkpoint.h5\")\n",
    "        self.target_model.set_weights(self.engine_model.get_weights()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 64)           576         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 32)           160         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 64)           256         dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 32)           128         dense_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 64)           4160        batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 32)           1056        batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 64)           256         dense_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 32)           128         dense_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 64)           4160        batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 32)           1056        batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 64)           256         dense_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 32)           128         dense_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 96)           0           batch_normalization_37[0][0]     \n",
      "                                                                 batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_43 (Dense)                (None, 64)           6208        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 64)           256         dense_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_44 (Dense)                (None, 32)           2080        batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 32)           128         dense_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_45 (Dense)                (None, 1)            33          batch_normalization_42[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 21,025\n",
      "Trainable params: 20,257\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_state = Input(shape=(8,))\n",
    "input_action = Input(shape=(4,))\n",
    "\n",
    "m = Dense(64, activation='relu')(input_state)\n",
    "m = BatchNormalization()(m)\n",
    "\n",
    "m = Dense(64, activation='relu')(m)\n",
    "m = BatchNormalization()(m)\n",
    "\n",
    "m = Dense(64, activation='relu')(m)\n",
    "m = BatchNormalization()(m)\n",
    "\n",
    "# m = Model(inputs = input_state, outputs = m)\n",
    "\n",
    "#####\n",
    "\n",
    "m_i = Dense(32,activation='relu')(input_action)\n",
    "m_i = BatchNormalization()(m_i)\n",
    "\n",
    "m_i = Dense(32,activation='relu')(m_i)\n",
    "m_i = BatchNormalization()(m_i)\n",
    "\n",
    "m_i = Dense(32,activation='relu')(m_i)\n",
    "m_i = BatchNormalization()(m_i)\n",
    "\n",
    "# m_i = Model(inputs = input_action, outputs = m_i)\n",
    "\n",
    "u = concatenate([m,m_i])\n",
    "\n",
    "u = Dense(64, activation='relu')(u)\n",
    "u = BatchNormalization()(u)\n",
    "\n",
    "u = Dense(32, activation='relu')(u)\n",
    "u = BatchNormalization()(u)\n",
    "\n",
    "u = Dense(1)(u)\n",
    "\n",
    "u = Model(inputs= [input_state,input_action ], outputs = u)\n",
    "\n",
    "u.compile(loss='categorical_crossentropy', optimizer=Adam())\n",
    "u.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "state_size = 8\n",
    "action_size = 4\n",
    "agent = DQNAgent(state_size, action_size, memsize= 9000)\n",
    "\n",
    "agent.target_model = m\n",
    "agent.engine_model = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "batch_size = 30\n",
    "game_history = [0]\n",
    "\n",
    "\n",
    "explore = True\n",
    "t_steps = 0\n",
    "for episode in range(3000):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    start = time.time()\n",
    "    for timee in range(450):\n",
    "\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        total_reward += reward\n",
    "\n",
    "    game_history.append(total_reward)\n",
    "    \n",
    "    if episode % 30 == 29:\n",
    "#         print(\"epi:{} score: {} mean:{} spent:{}\".format(episode, total_reward, np.mean(game_history[-30:]), time.time() - start))\n",
    "        agent.learn()\n",
    "        agent.save_model()\n",
    "        np.save(\"game_history\", game_history)\n",
    "        pl.figure(figsize=(40,10))\n",
    "        pl.plot(game_history)\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(pl.gcf())\n",
    "import time\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
