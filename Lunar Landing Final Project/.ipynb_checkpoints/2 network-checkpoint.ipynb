{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "# from  lunarLanding import DQNAgent\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Conv1D, MaxPooling1D, Flatten, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())\n",
    "\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from scipy.special import softmax\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "import time\n",
    "\n",
    "import os\n",
    "\n",
    "# Deep Q-learning Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, memsize = 3000, ga = 0.95, explore_rate = 1, explore_decay = 0.9995):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen = memsize)\n",
    "        self.gamma = ga    # discount rate\n",
    "        self.epsilon = explore_rate  # exploration rate\n",
    "        self.epsilon_min = 0.45\n",
    "        self.epsilon_decay = explore_decay\n",
    "        self.target_model = Sequential()\n",
    "        self.engine_model = Sequential()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        if not done:\n",
    "            self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            chose = np.random.randint(0,4)\n",
    "            return chose\n",
    "        \n",
    "        act_values = 0\n",
    "        act_values = self.target_model.predict(state)\n",
    "        chose = np.argmax(act_values)\n",
    "        if chose == 0:\n",
    "            return chose\n",
    "        if chose == 1:\n",
    "            return chose\n",
    "        if chose == 2:\n",
    "            return chose\n",
    "        if chose == 3:\n",
    "            return chose\n",
    "        return act_values\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        xs = []\n",
    "        ys = []\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "\n",
    "            if not done:\n",
    "                target = reward + np.multiply (self.gamma , self.model.predict(next_state)[0] )\n",
    "            else:\n",
    "                target = np.multiply (self.gamma , self.model.predict(next_state)[0] )\n",
    "                \n",
    "            xs.append(state[0])\n",
    "            ys.append(target)\n",
    "        xs = np.array(xs)\n",
    "        ys = np.array(ys)\n",
    "        self.model.fit(xs, ys, epochs= 1, verbose=0 , batch_size=batch_size)\n",
    "                \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def save_model(self, model_name = './checkpoint.h5', mem_name = 'memory'):\n",
    "        self.model.save(model_name)\n",
    "        np.save(mem_name , self.memory)\n",
    "\n",
    "    def load_model(self,  model_name = './checkpoint.h5' , mem_name = 'memory.npy'):\n",
    "        self.model.load_weights(model_name)\n",
    "        self.memory = np.load(mem_name, allow_pickle=True)\n",
    "        self.memory = deque(self.memory)\n",
    "    def learn (self):\n",
    "        xs = self.memory[:][0]\n",
    "        ys = self.memory[:][3]\n",
    "        self.engine_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "state_size = 8\n",
    "action_size = 4\n",
    "agent = DQNAgent(state_size, action_size, memsize= 2500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
