{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from  lunarLanding import DQNAgent\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Conv1D, MaxPooling1D, Flatten, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())\n",
    "\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "state_size = 8\n",
    "action_size = 4\n",
    "agent = DQNAgent(state_size, action_size, memsize= 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 50)                450       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4)                 204       \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 4)                 16        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 6,370\n",
      "Trainable params: 6,062\n",
      "Non-trainable params: 308\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(50, input_dim = 8))\n",
    "m.add(BatchNormalization())\n",
    "\n",
    "m.add(Dense(50))\n",
    "m.add(Activation('relu'))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "m.add(Dense(50))\n",
    "m.add(Activation('relu'))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "m.add(Dense(4))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Activation('softmax'))\n",
    "\n",
    "m.compile(loss='categorical_crossentropy', optimizer=Adam())\n",
    "m.summary()\n",
    "\n",
    "agent.model = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epi:0 score: 10.364088282095853spent:0.025298595428466797\n",
      "epi:1 score: -25.69798437469811spent:0.02034592628479004\n",
      "epi:2 score: 19.59815959632649spent:0.009768486022949219\n",
      "epi:3 score: 25.962703030712767spent:0.007816314697265625\n",
      "epi:4 score: -177.83472213517237spent:0.015173912048339844\n",
      "epi:5 score: 4.101419024338433spent:0.011360645294189453\n",
      "epi:6 score: -2.8252981458069213spent:0.01207113265991211\n",
      "epi:7 score: -13.493203453028071spent:0.012637853622436523\n",
      "epi:8 score: -287.71521035111306spent:0.014567375183105469\n",
      "epi:9 score: -410.94897134461627spent:0.01723623275756836\n",
      "epi:10 score: -218.78741450136272spent:0.01506352424621582\n",
      "epi:11 score: -248.49161162185294spent:0.02084660530090332\n",
      "epi:12 score: 41.05042433334083spent:0.011678695678710938\n",
      "epi:13 score: -93.42741819533052spent:0.018170833587646484\n",
      "epi:14 score: -18.613194777172044spent:0.009783744812011719\n",
      "epi:15 score: -1.4930299748514777spent:0.012402534484863281\n",
      "epi:16 score: -41.621195165273264spent:0.010607719421386719\n",
      "epi:17 score: -185.8668836363966spent:0.016611814498901367\n",
      "epi:18 score: 12.491979641469026spent:0.014080524444580078\n",
      "epi:19 score: 73.7235641257038spent:0.015901803970336914\n",
      "epi:20 score: -18.92244630384858spent:0.010767936706542969\n",
      "epi:21 score: -239.3734033839532spent:0.01362466812133789\n",
      "epi:22 score: -70.70433877345879spent:7.058636903762817\n",
      "epi:23 score: -138.3676043947037spent:3.761608362197876\n",
      "epi:24 score: 23.022120121188973spent:4.579213619232178\n",
      "epi:25 score: -34.56470331868669spent:4.201484680175781\n",
      "epi:26 score: 6.509689462493796spent:4.54719614982605\n",
      "epi:27 score: -140.64662779634907spent:4.395758628845215\n",
      "epi:28 score: -98.03171979999061spent:5.444608449935913\n",
      "epi:29 score: -64.17825638521963spent:3.7223668098449707\n",
      "epi:30 score: -155.23304937394124spent:6.0115885734558105\n",
      "epi:31 score: 25.684364733288536spent:5.657802581787109\n",
      "epi:32 score: -90.24916412150175spent:3.0122363567352295\n",
      "epi:33 score: -186.687643948259spent:4.5489661693573\n",
      "epi:34 score: -216.64581829914587 mean:-88.74685611465151 spent:3.602099657058716\n",
      "epi:35 score: -275.19683041535376 mean:-96.35882981546129 spent:8.703680515289307\n",
      "epi:36 score: -79.88969336244396 mean:-105.75095161785946 spent:3.2869629859924316\n",
      "epi:37 score: -358.9053731451208 mean:-108.04048575266691 spent:5.080908298492432\n",
      "epi:38 score: 16.900866546893333 mean:-110.49531895246027 spent:4.774654388427734\n",
      "epi:39 score: -571.4049493581641 mean:-95.74187626654616 spent:8.940279722213745\n",
      "epi:40 score: -520.7277438718154 mean:-107.90110160643586 spent:11.274606704711914\n",
      "epi:41 score: 15.47686841935023 mean:-117.28855444264144 spent:2.944655418395996\n",
      "epi:42 score: -621.4740324182094 mean:-118.17040119829629 spent:8.353688478469849\n",
      "epi:43 score: -268.672596736274 mean:-136.37890513701626 spent:4.030176401138306\n",
      "epi:44 score: -17.292182356358936 mean:-145.00164313560597 spent:2.7103846073150635\n",
      "epi:45 score: -583.1148006763833 mean:-145.54644149358901 spent:8.256520748138428\n",
      "epi:46 score: -3.4071637439313944 mean:-164.2186347870756 spent:2.796492099761963\n",
      "epi:47 score: -1.8428757096864175 mean:-157.92692030802502 spent:4.276299476623535\n",
      "epi:48 score: -779.6139780932884 mean:-158.42122566496138 spent:8.012282848358154\n",
      "epi:49 score: -1078.6984584604097 mean:-187.84665815527143 spent:11.229209899902344\n",
      "epi:50 score: 97.35297199922675 mean:-224.3906585744632 spent:4.980316162109375\n",
      "epi:51 score: -85.81116199876972 mean:-212.77940425090526 spent:4.458161354064941\n",
      "epi:52 score: -44.96227455554697 mean:-213.30032918970912 spent:2.7827301025390625\n",
      "epi:53 score: -156.99558251072716 mean:-210.07945574697956 spent:4.046175003051758\n",
      "epi:54 score: -22.215349393810182 mean:-216.28696273428704 spent:3.236267566680908\n",
      "epi:55 score: -72.43956939708649 mean:-215.8611229437741 spent:2.7992584705352783\n",
      "epi:56 score: -58.422528032241104 mean:-218.58351118031132 spent:3.7740931510925293\n",
      "epi:57 score: -51.4786005105418 mean:-215.7481973953421 spent:3.3497061729431152\n",
      "epi:58 score: -106.52260708370548 mean:-214.14291741984383 spent:3.7622241973876953\n",
      "epi:59 score: -47.98937003926487 mean:-215.60306744392958 spent:3.944361686706543\n",
      "epi:60 score: -70.94038208277331 mean:-211.90500953583728 spent:3.14591646194458\n",
      "epi:61 score: -22.75347732843477 mean:-215.23689735708078 spent:4.008275508880615\n",
      "epi:62 score: -76.61315406721621 mean:-212.9094598814578 spent:3.3404674530029297\n",
      "epi:63 score: -28.026583153488943 mean:-209.11378781659425 spent:3.762259006500244\n",
      "epi:64 score: -19.673819281791076 mean:-202.60967625984745 spent:3.4687912464141846\n",
      "epi:65 score: -18.728956573484812 mean:-193.79853794489702 spent:3.3531742095947266\n",
      "epi:66 score: 1.5669014989524293 mean:-191.68954702113976 spent:2.686464309692383\n",
      "epi:67 score: -0.24619750549755182 mean:-179.25946858513726 spent:3.0608112812042236\n",
      "epi:68 score: -288.9567613957794 mean:-179.85074665590938 spent:5.573444843292236\n",
      "epi:69 score: -493.75432615335455 mean:-170.11115396755127 spent:6.935843467712402\n",
      "epi:70 score: -687.154243079157 mean:-169.18103611519055 spent:7.189107179641724\n",
      "epi:71 score: -22.808658871911362 mean:-193.40969513238048 spent:3.0400524139404297\n",
      "epi:72 score: -296.22791485058724 mean:-172.76606156181847 spent:5.925621509552002\n",
      "epi:73 score: -121.39886866513976 mean:-173.71624494507066 spent:4.854280948638916\n",
      "epi:74 score: -196.48635728457307 mean:-177.3061306798562 spent:3.651808738708496\n",
      "epi:75 score: -417.36439177789515 mean:-163.97411539048343 spent:5.501718044281006\n",
      "epi:76 score: -378.9907054403634 mean:-178.24850256406845 spent:4.189110279083252\n",
      "epi:77 score: -303.06271587520786 mean:-191.25360014098837 spent:4.541668176651001\n",
      "epi:78 score: -760.933632439517 mean:-174.8207979955372 spent:9.18195652961731\n",
      "epi:79 score: 63.93456712768085 mean:-163.86339020171332 spent:3.6179568767547607\n",
      "epi:80 score: -561.5204589587715 mean:-165.01574899038735 spent:5.4291486740112305\n",
      "epi:81 score: -337.2171470829523 mean:-181.41951785107705 spent:5.374932527542114\n",
      "epi:82 score: -162.84198785895742 mean:-191.49727207616004 spent:4.164414644241333\n",
      "epi:83 score: -677.5014833190133 mean:-191.69887226058174 spent:5.58364725112915\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9b22db2cebd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mexplore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#         print(\"diff:\", reward - prev_reward,\"prev:\", prev_reward, \" current:\", reward, \"total reward:\", total_reward)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MLDL/Lunar Landing Final Project/lunarLanding.py\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "done = False\n",
    "batch_size = 30\n",
    "game_history = [0]\n",
    "\n",
    "\n",
    "explore = True\n",
    "t_steps = 0\n",
    "for episode in range(300):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "#     print(state.shape)\n",
    "    total_reward = 0\n",
    "    prev_reward = 0\n",
    "    start = time.time()\n",
    "    for timee in range(450):\n",
    "        # env.render()\n",
    "#         if time % 5 == 0:\n",
    "#             print(time, end=', ')\n",
    "        \n",
    "\n",
    "#         print(state)\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        prev_reward = reward\n",
    "        total_reward += reward\n",
    "        if len(agent.memory) > 2000:\n",
    "            agent.replay(batch_size)\n",
    "            explore = False\n",
    "#         print(\"diff:\", reward - prev_reward,\"prev:\", prev_reward, \" current:\", reward, \"total reward:\", total_reward)\n",
    "\n",
    "\n",
    "    game_history.append(total_reward)\n",
    "#     if explore == False:\n",
    "    if len(game_history) > 35:\n",
    "        print(\"epi:{} score: {} mean:{} spent:{}\".format(episode, total_reward, np.mean(game_history[-30:-1]), time.time() - start))\n",
    "    else:\n",
    "        print(\"epi:{} score: {}spent:{}\".format(episode, total_reward, time.time() - start))\n",
    "        \n",
    "    if episode % 10 == 9:\n",
    "        agent.save_model()\n",
    "        np.save(\"game_history\", game_history)\n",
    "\n",
    "    done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
