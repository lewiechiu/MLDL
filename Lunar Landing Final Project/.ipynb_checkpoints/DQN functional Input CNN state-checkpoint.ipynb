{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "# from  lunarLanding import DQNAgent\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Activation, Input, concatenate, add\n",
    "from keras.layers import Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())\n",
    "\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "import itertools\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from scipy.special import softmax\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "import time\n",
    "\n",
    "import os\n",
    "\n",
    "# Deep Q-learning Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, memsize = 7000, ga = 0.95, explore_rate = 1, explore_decay = 0.995):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen = memsize)\n",
    "        self.gamma = ga    # discount rate\n",
    "        self.epsilon = explore_rate  # exploration rate\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = explore_decay\n",
    "        self.target_model = Sequential()\n",
    "        self.engine_model = Sequential()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        if not done:\n",
    "            self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "                \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            chose = np.random.randint(0,4)\n",
    "            return chose\n",
    "        \n",
    "        ACTION_SIZE = 4\n",
    "        STATE_SIZE = 8\n",
    "        bestStep = 0\n",
    "        initialStep = np_utils.to_categorical(0, ACTION_SIZE).reshape(1, ACTION_SIZE)\n",
    "        best_reward = self.target_model.predict([state.reshape(1, STATE_SIZE), initialStep])\n",
    "        for action in range(1, ACTION_SIZE):\n",
    "            step = np_utils.to_categorical(action, ACTION_SIZE).reshape(1, ACTION_SIZE)\n",
    "            predict = self.target_model.predict([state.reshape(1, STATE_SIZE), step])\n",
    "            if(predict > best_reward):\n",
    "                bestStep = action\n",
    "                best_reward = predict\n",
    "                \n",
    "        return bestStep\n",
    "#     def replay(self, batch_size):\n",
    "#         minibatch = random.sample(self.memory, batch_size)\n",
    "#         xs = []\n",
    "#         ys = []\n",
    "\n",
    "#         for state, action, reward, next_state, done in minibatch:\n",
    "#             target = reward\n",
    "\n",
    "#             if not done:\n",
    "#                 target = reward + np.multiply (self.gamma , self.model.predict(next_state)[0] )\n",
    "#             else:\n",
    "#                 target = np.multiply (self.gamma , self.model.predict(next_state)[0] )\n",
    "                \n",
    "#             xs.append(state[0])\n",
    "#             ys.append(target)\n",
    "#         xs = np.array(xs)\n",
    "#         ys = np.array(ys)\n",
    "#         self.model.fit(xs, ys, epochs= 1, verbose=0 , batch_size=batch_size)\n",
    "                \n",
    "#         if self.epsilon > self.epsilon_min:\n",
    "#             self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def save_model(self, model_name = './checkpoint.h5', mem_name = 'memory'):\n",
    "        self.target_model.save(model_name)\n",
    "\n",
    "    def load_model(self,  model_name = './checkpoint.h5' , mem_name = 'memory.npy'):\n",
    "        self.target_model.load_weights(model_name)\n",
    "        self.engine_model.load_weights(model_name)\n",
    "        self.memory = np.load(mem_name, allow_pickle=True)\n",
    "        self.memory = deque(self.memory)\n",
    "    def learn (self):\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay** 10\n",
    "        output = list(itertools.islice(agent.memory, 0, None))\n",
    "        xs_state = []\n",
    "        xs_action = []\n",
    "        ys = []\n",
    "        for i in output:\n",
    "            ys.append(i[2])\n",
    "#             print('reward',i[2])\n",
    "            step = np_utils.to_categorical(i[1], 4).reshape(1, 4)\n",
    "\n",
    "            xs_state.append(i[0][0])\n",
    "            xs_action.append(step)\n",
    "\n",
    "        ys = np.array(ys)\n",
    "        lgn = len(xs_state)\n",
    "        xs_action = np.array(xs_action).reshape(lgn, 4)\n",
    "#         print(xs.shape)\n",
    "        self.engine_model.fit([xs_state, xs_action], ys, epochs = 4 , verbose = 1)\n",
    "#         del self.memory\n",
    "        \n",
    "#         self.memory = deque(maxlen = 15000)\n",
    "#         print(len(self.memory))\n",
    "        self.engine_model.save_weights(\"checkpoint.h5\")\n",
    "        self.target_model.set_weights(self.engine_model.get_weights()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_75 (InputLayer)           (None, 8, 1)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 8, 32)        128         input_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_76 (InputLayer)           (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 2, 32)        0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_267 (Dense)               (None, 32)           160         input_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 64)           0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_239 (BatchN (None, 32)           128         dense_267[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_265 (Dense)               (None, 64)           4160        flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_268 (Dense)               (None, 32)           1056        batch_normalization_239[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_237 (BatchN (None, 64)           256         dense_265[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_240 (BatchN (None, 32)           128         dense_268[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_266 (Dense)               (None, 32)           2080        batch_normalization_237[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_269 (Dense)               (None, 32)           1056        batch_normalization_240[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_238 (BatchN (None, 32)           128         dense_266[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_241 (BatchN (None, 32)           128         dense_269[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 64)           0           batch_normalization_238[0][0]    \n",
      "                                                                 batch_normalization_241[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_270 (Dense)               (None, 64)           4160        concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_242 (BatchN (None, 64)           256         dense_270[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_271 (Dense)               (None, 32)           2080        batch_normalization_242[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_243 (BatchN (None, 32)           128         dense_271[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_272 (Dense)               (None, 1)            33          batch_normalization_243[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 16,065\n",
      "Trainable params: 15,489\n",
      "Non-trainable params: 576\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_state = Input(shape=(8,1))\n",
    "input_action = Input(shape=(4,))\n",
    "\n",
    "m = Conv1D(filters=32, kernel_size=3,padding='same', activation='relu')(input_state)\n",
    "m = Conv1D(filters=32, kernel_size=3,padding='same')(input_state)\n",
    "m = MaxPooling1D(3)(m)\n",
    "\n",
    "m = Conv1D(filters=32, kernel_size=3,padding='same')(input_state)\n",
    "m = Conv1D(filters=32, kernel_size=3,padding='same')(input_state)\n",
    "m = MaxPooling1D(3)(m)\n",
    "\n",
    "m = Flatten()(m)\n",
    "\n",
    "\n",
    "m = Dense(64, activation='relu')(m)\n",
    "m = BatchNormalization()(m)\n",
    "\n",
    "m = Dense(32, activation='relu')(m)\n",
    "m = BatchNormalization()(m)\n",
    "\n",
    "# m = Model(inputs = input_state, outputs = m)\n",
    "\n",
    "#####\n",
    "\n",
    "m_i = Dense(32,activation='relu')(input_action)\n",
    "m_i = BatchNormalization()(m_i)\n",
    "\n",
    "m_i = Dense(32,activation='relu')(m_i)\n",
    "m_i = BatchNormalization()(m_i)\n",
    "\n",
    "m_i = Dense(32,activation='relu')(m_i)\n",
    "m_i = BatchNormalization()(m_i)\n",
    "\n",
    "# m_i = Model(inputs = input_action, outputs = m_i)\n",
    "\n",
    "u = concatenate([m,m_i])\n",
    "\n",
    "u = Dense(64, activation='relu')(u)\n",
    "u = BatchNormalization()(u)\n",
    "\n",
    "u = Dense(32, activation='relu')(u)\n",
    "u = BatchNormalization()(u)\n",
    "\n",
    "u = Dense(1)(u)\n",
    "\n",
    "u = Model(inputs= [input_state,input_action ], outputs = u)\n",
    "\n",
    "u.compile(loss='mse', optimizer=Adam())\n",
    "u.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "state_size = 8\n",
    "action_size = 4\n",
    "agent = DQNAgent(state_size, action_size, memsize= 15000)\n",
    "\n",
    "agent.target_model = u\n",
    "agent.engine_model = u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_73 to have 3 dimensions, but got array with shape (1, 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-844b4baeb9c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtimee\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m450\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-134-efab55a32368>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mbestStep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0minitialStep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mACTION_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mACTION_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mbest_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTATE_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialStep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mACTION_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mACTION_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mACTION_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_73 to have 3 dimensions, but got array with shape (1, 8)"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "batch_size = 30\n",
    "game_history = [0]\n",
    "\n",
    "\n",
    "explore = True\n",
    "t_steps = 0\n",
    "for episode in range(3000):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    start = time.time()\n",
    "    for timee in range(450):\n",
    "\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        total_reward += reward\n",
    "\n",
    "    game_history.append(total_reward)\n",
    "    \n",
    "    if episode % 50 == 49:\n",
    "        print(\"epi:{} score: {} mean:{} spent:{}\".format(episode, total_reward, np.mean(game_history[-50:]), time.time() - start))\n",
    "        agent.learn()\n",
    "        agent.save_model()\n",
    "        np.save(\"game_history\", game_history)\n",
    "        pl.figure(figsize=(40,20))\n",
    "        pl.plot(game_history)\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(pl.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
