{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from lunarLanding import DQNAgent\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Conv1D, MaxPooling1D, Flatten, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())\n",
    "\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "state_size = 8\n",
    "action_size = 4\n",
    "agent = DQNAgent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_61 (Dense)             (None, 100)               900       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 32)                1632      \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 4)                 132       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 7,714\n",
      "Trainable params: 7,714\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(100, input_dim = 8))\n",
    "m.add(Activation('elu'))\n",
    "\n",
    "m.add(Dense(50))\n",
    "m.add(Activation('elu'))\n",
    "\n",
    "m.add(Dense(32))\n",
    "m.add(Activation('elu'))\n",
    "\n",
    "m.add(Dense(4))\n",
    "m.add(Activation('softmax'))\n",
    "\n",
    "m.compile(loss='categorical_crossentropy', optimizer=Adam(lr = 0.0001))\n",
    "m.summary()\n",
    "\n",
    "agent.model = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epi:0 score: -28.62593769787399spent:0.013043403625488281\n",
      "epi:1 score: 14.684772251046905spent:0.009779214859008789\n",
      "epi:2 score: -273.9855349451718spent:0.011420726776123047\n",
      "epi:3 score: -158.03376120105926spent:0.016477584838867188\n",
      "epi:4 score: -177.9290876588574spent:0.01842975616455078\n",
      "epi:5 score: -14.389761449934111spent:0.015700578689575195\n",
      "epi:6 score: -119.08245512627458spent:0.019420385360717773\n",
      "epi:7 score: -209.67786894992963spent:0.019994258880615234\n",
      "epi:8 score: 81.16347448353785spent:0.013658285140991211\n",
      "epi:9 score: -93.58004419880008spent:0.012328147888183594\n",
      "epi:10 score: -177.08894294511433spent:0.01501774787902832\n",
      "epi:11 score: -240.27343709115317spent:0.013387441635131836\n",
      "epi:12 score: -3.5259187707101987spent:0.018529176712036133\n",
      "epi:13 score: -41.368709157105684spent:0.01035308837890625\n",
      "epi:14 score: -257.80402031507543spent:0.01998591423034668\n",
      "epi:15 score: -22.6442735832694spent:0.01681351661682129\n",
      "epi:16 score: 22.524781687165607spent:0.01569962501525879\n",
      "epi:17 score: -40.15305455793505spent:0.015486717224121094\n",
      "epi:18 score: 30.784014114924666spent:0.0089569091796875\n",
      "epi:19 score: -154.7861889163535spent:0.016025066375732422\n",
      "epi:20 score: -3.797365732199793spent:0.015564203262329102\n",
      "epi:21 score: -262.77210707715324spent:5.570533037185669\n",
      "epi:22 score: -244.9009108744758spent:4.549280881881714\n",
      "epi:23 score: -202.71406642661142spent:3.512392044067383\n",
      "epi:24 score: -14.210466013546226spent:2.1852035522460938\n",
      "epi:25 score: -266.46407617293914spent:4.845277309417725\n",
      "epi:26 score: -370.8592216069343spent:6.351989507675171\n",
      "epi:27 score: 24.62033792981653spent:3.872897148132324\n",
      "epi:28 score: -181.82742295766624spent:3.5019171237945557\n",
      "epi:29 score: -65.01466022858477spent:4.852170705795288\n",
      "epi:30 score: -300.1516742919371spent:3.230456829071045\n",
      "epi:31 score: -213.68800880316456spent:3.880582571029663\n",
      "epi:32 score: -296.426230185045spent:3.5139949321746826\n",
      "epi:33 score: -336.2660721685096spent:3.1277894973754883\n",
      "epi:34 score: -399.7821701065045 mean:-137.0473913581027 spent:4.550541162490845\n",
      "epi:35 score: -319.9802794953943 mean:-150.33678476005335 spent:4.926802635192871\n",
      "epi:36 score: -465.6338238534221 mean:-157.2642959451954 spent:4.808434963226318\n",
      "epi:37 score: -314.36910075003476 mean:-166.09036335566068 spent:3.908205270767212\n",
      "epi:38 score: -512.9517666781273 mean:-179.72941767405973 spent:3.8499667644500732\n",
      "epi:39 score: -332.8146951746202 mean:-194.19051155265723 spent:2.9089608192443848\n",
      "epi:40 score: -591.4447195037656 mean:-199.5603650778126 spent:3.9813473224639893\n",
      "epi:41 score: -805.3472657485933 mean:-211.66971964376475 spent:6.8435378074646\n",
      "epi:42 score: -512.8294031654648 mean:-239.31873160851936 spent:3.6132891178131104\n",
      "epi:43 score: -371.3845704333031 mean:-255.57599691915246 spent:3.491084098815918\n",
      "epi:44 score: -1045.0119817534394 mean:-259.49256761288444 spent:7.588538408279419\n",
      "epi:45 score: -881.014326547194 mean:-294.74662651530406 spent:7.994585275650024\n",
      "epi:46 score: -213.83162074179126 mean:-325.90314748890273 spent:3.440539598464966\n",
      "epi:47 score: -356.3864238548862 mean:-331.8920635642081 spent:3.603109836578369\n",
      "epi:48 score: -389.5693531904524 mean:-345.2427683217878 spent:5.295124769210815\n",
      "epi:49 score: -870.577342474336 mean:-353.3387395036533 spent:9.6040358543396\n",
      "epi:50 score: -315.6471619798035 mean:-383.22770421889936 spent:5.353300333023071\n",
      "epi:51 score: -337.8486148605342 mean:-385.05098197416316 spent:2.9344162940979004\n",
      "epi:52 score: -236.76343799810562 mean:-388.2560752150617 spent:5.310709714889526\n",
      "epi:53 score: -1002.161536362316 mean:-389.4301914761477 spent:7.6763715744018555\n",
      "epi:54 score: -414.01425363660655 mean:-423.4974697640363 spent:2.488757371902466\n",
      "epi:55 score: -299.38821721816026 mean:-428.585406917956 spent:3.3802807331085205\n",
      "epi:56 score: -672.6148208380746 mean:-426.12088952523953 spent:4.4859490394592285\n",
      "epi:57 score: -186.08679235357098 mean:-450.16348120689105 spent:3.588937997817993\n",
      "epi:58 score: -295.80994236195636 mean:-450.3103560136464 spent:4.090741157531738\n",
      "epi:59 score: -470.584473301677 mean:-458.2688140182454 spent:2.6416304111480713\n",
      "epi:60 score: -334.30388669073017 mean:-464.14580708754676 spent:4.103216171264648\n",
      "epi:61 score: -514.9265161737035 mean:-468.3049752905662 spent:5.153198957443237\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "batch_size = 30\n",
    "game_history = [0]\n",
    "\n",
    "\n",
    "explore = True\n",
    "t_steps = 0\n",
    "for episode in range(300):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "#     print(state.shape)\n",
    "    total_reward = 0\n",
    "    prev_reward = 0\n",
    "    start = time.time()\n",
    "    for timee in range(450):\n",
    "        # env.render()\n",
    "#         if time % 5 == 0:\n",
    "#             print(time, end=', ')\n",
    "        \n",
    "\n",
    "#         print(state)\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        prev_reward = reward\n",
    "        total_reward += reward\n",
    "        if len(agent.memory) > 2000:\n",
    "            agent.replay(batch_size)\n",
    "            explore = False\n",
    "#         print(\"diff:\", reward - prev_reward,\"prev:\", prev_reward, \" current:\", reward, \"total reward:\", total_reward)\n",
    "\n",
    "\n",
    "    game_history.append(total_reward)\n",
    "#     if explore == False:\n",
    "    if len(game_history) > 35:\n",
    "        print(\"epi:{} score: {} mean:{} spent:{}\".format(episode, total_reward, np.mean(game_history[-30:-1]), time.time() - start))\n",
    "    else:\n",
    "        print(\"epi:{} score: {}spent:{}\".format(episode, total_reward, time.time() - start))\n",
    "        \n",
    "    if episode % 10 == 9:\n",
    "        agent.save_model()\n",
    "        np.save(\"game_history\", game_history)\n",
    "\n",
    "    done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: ([0, 0, 0, 0.8],)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-1e90300dd4a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.03\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mlog_loss\u001b[0;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   2126\u001b[0m         \u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2127\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2128\u001b[0;31m         \u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_input_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0m_unique_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FN_UNIQUE_LABELS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_unique_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mys_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: ([0, 0, 0, 0.8],)"
     ]
    }
   ],
   "source": [
    "log_loss([0,0,0,0.8],[0.9,0.05,0.02,0.03])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
