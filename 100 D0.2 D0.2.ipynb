{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.3) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "# print(K.tensorflow_backend._get_available_gpus())\n",
    "\n",
    "\n",
    "from scipy.special import softmax\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-learning Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen = 2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1  # exploration rate\n",
    "        self.epsilon_min = 0.15\n",
    "        self.epsilon_decay = 0.9995\n",
    "        self.model = self._build_model()\n",
    "        self.walk = deque(maxlen = 900)\n",
    "        \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(100, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(100, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.2))\n",
    "        \n",
    "        \n",
    "        \n",
    "        model.add(Dense(100, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(self.action_size, activation='softmax'))\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adam())\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "#         print(state)\n",
    "#         print(reward)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            chose = np.random.randint(0,4)\n",
    "            return chose\n",
    "        \n",
    "        act_values = 0\n",
    "        act_values = self.model.predict(state)\n",
    "        chose = np.argmax(act_values)\n",
    "#         print(act_values, chose)\n",
    "        if chose == 0:\n",
    "            return chose\n",
    "        if chose == 1:\n",
    "            return chose\n",
    "        if chose == 2:\n",
    "            return chose\n",
    "        if chose == 3:\n",
    "            return chose\n",
    "        return act_values\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        xs = []\n",
    "        ys = []\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "\n",
    "            if not done:\n",
    "#                 if reward >= 0:\n",
    "                \n",
    "                target = reward + np.multiply (self.gamma , self.model.predict(next_state)[0] )\n",
    "#                 else:\n",
    "#                     target = np.multiply (-1 , self.model.predict(next_state)[0] )\n",
    "            else:\n",
    "                target = np.multiply (self.gamma , self.model.predict(next_state)[0] )\n",
    "#             print(target)\n",
    "\n",
    "            xs.append(state[0])\n",
    "\n",
    "            ys.append(target)\n",
    "#             ys = np.array(ys)\n",
    "#             print(ys.shape)\n",
    "        xs = np.array(xs)\n",
    "        ys = np.array(ys)\n",
    "        self.model.fit(xs, ys, epochs= 1, verbose=0 , batch_size=batch_size)\n",
    "                \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        random.shuffle(self.memory)\n",
    "#         self.memory = deque(self.memory)\n",
    "        \n",
    "        \n",
    "        if np.random.rand() > 0.8:\n",
    "            self.memory.pop()\n",
    "\n",
    "    \n",
    "    def save_model(self):\n",
    "        self.model.save('./checkpoint.h5')\n",
    "        np.save(\"game_memory\", self.memory)\n",
    "\n",
    "    def load_model(self):\n",
    "        self.model.load_weights('./checkpoint.h5')\n",
    "        self.memory = np.load(\"game_memory.npy\", allow_pickle=True)\n",
    "        self.memory = deque(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
