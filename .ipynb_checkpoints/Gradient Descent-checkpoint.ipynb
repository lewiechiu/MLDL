{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this kernel, I will implement a few gradient descent techniques.\n",
    "\n",
    "The main function for me to optimize will be $$a^T\\vec{x}=\\vec{y}$$. The goal for SGD is to optimize $a$, such that it has the smallest loss. Here, $x$ and $y$ will be randomly decided. $x$ will be a matrix of $5000\\times30$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data\n",
    "We will first generate the bias matrix ($a$), then $x$. $y$ will be acquired by $a^Tx$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1430 3850 5648 4367 4363 1962 5762 6793  774 3129 6884 7927 2186 2802\n",
      " 5505 8225 3693 2870 8940 4301 1570  787 7338 7314 4207 2602 3419 5234\n",
      " 9437 9872 7455 2553  645 6050 7391 2526 9474 8963 3008 7390  465 2556\n",
      " 3162 3991 8864 4153 7976 6950 2636 2934 6886 7659 7603 5286 6707 2900\n",
      " 3434 3754 3603 2874 6785 7397 9187 6599 1834  942 6434 5896 1821 8180\n",
      " 2027 5445 9723 2142  738 9724 4054 8878 5447 8698 3545 3992 1158 3200\n",
      " 6224 3984 9621  865 6265  658 6647 1983 3094 5038 2001  454 2878 1987\n",
      "  616 7871]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randint(1,10000, size=100)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.37075035, -0.91197099,  0.89154802, ...,  0.73599573,\n",
       "         1.29935704, -0.46618493],\n",
       "       [ 0.23587862,  0.16087673, -0.3859583 , ..., -0.30686093,\n",
       "        -1.06823188, -0.71587608],\n",
       "       [ 0.39458593, -0.59904851, -0.1008914 , ...,  0.38214607,\n",
       "        -0.59972414, -1.094637  ],\n",
       "       ...,\n",
       "       [-0.24346001,  1.22900398,  0.89685907, ...,  0.15058909,\n",
       "        -0.65333525, -1.85719398],\n",
       "       [ 0.7336436 , -0.06278063,  0.94759201, ...,  0.11711587,\n",
       "        -1.15624659, -0.75118594],\n",
       "       [ 0.70682426, -0.48667389,  0.05296054, ..., -0.28594956,\n",
       "         0.22565602, -1.59264317]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randn(20000,100)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 22591.35388858,  39057.9468175 , -35418.14686145, ...,\n",
       "         3173.76795935,  -3548.10155834,   3044.33153944])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.zeros(20000)\n",
    "for i in range(len(y)):\n",
    "    y[i] = np.dot(a,x[i])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the data generated for our Gradient Descent problem. Lets move on\n",
    "\n",
    "## Loss Function and Hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y,predict):\n",
    "    summed = 1/(len(x)) * np.sum((predict-y)**2)\n",
    "    return summed\n",
    "\n",
    "alpha = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:1000\n",
      "cost404738711.7480242\n",
      "iter:2000\n",
      "cost54783745.971608646\n",
      "iter:3000\n",
      "cost7560788.582976928\n",
      "iter:4000\n",
      "cost1063226.8074276764\n",
      "iter:5000\n",
      "cost152199.36508150303\n",
      "iter:6000\n",
      "cost22152.697645343436\n",
      "iter:7000\n",
      "cost3274.2773604134572\n",
      "iter:8000\n",
      "cost490.8047072846749\n",
      "iter:9000\n",
      "cost74.51474597654853\n",
      "iter:10000\n",
      "cost11.443964567522594\n",
      "iter:11000\n",
      "cost1.7758578761611994\n",
      "iterations to reach cost < 1\n",
      "11310\n",
      "iter:12000\n",
      "cost0.27814805162902695\n",
      "iter:13000\n",
      "cost0.04393013264918493\n",
      "iter:14000\n",
      "cost0.006990257776670096\n",
      "iter:15000\n",
      "cost0.0011197831784630255\n",
      "iter:16000\n",
      "cost0.00018046276911929242\n",
      "iter:17000\n",
      "cost2.9240874482493696e-05\n",
      "iterations spent to find optimum:\n",
      "17591\n",
      "time spent\n",
      "371.24827671051025\n"
     ]
    }
   ],
   "source": [
    "theta = np.zeros(100)\n",
    "cost = 1\n",
    "hist_cost = []\n",
    "printed = True\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "ite = 0 \n",
    "while cost > 0.00001:\n",
    "    ite +=1\n",
    "    for j in range(100):\n",
    "        theta[j] = theta[j] - alpha / len(x) *np.sum(  (x @ theta.T - y) @ x[:,j], axis=0)\n",
    "        \n",
    "    cost = loss(y, x @ theta.T)\n",
    "    hist_cost.append(cost)\n",
    "    if ite % 1000 == 0:\n",
    "        print(\"iter:\",end='')\n",
    "        print(ite)\n",
    "        print(\"cost\",end='')\n",
    "        print(cost)\n",
    "    if cost <= 1 and printed:\n",
    "        printed = False\n",
    "        print(\"iterations to reach cost < 1\")\n",
    "        print(len(hist_cost))\n",
    "        \n",
    "\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(\"iterations spent to find optimum:\")\n",
    "print(len(hist_cost))\n",
    "print(\"time spent\")\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how close $\\theta$ and the real $a$ is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1430.00000401 3850.00022235 5647.99996937 4367.00006695 4363.00047009\n",
      " 1962.00006897 5761.99992474 6792.99989641  773.99972891 3128.99971262\n",
      " 6883.99997556 7927.00009965 2185.9998544  2802.00006615 5504.99984125\n",
      " 8224.99973813 3692.99937934 2869.9998837  8939.99967722 4300.99989305\n",
      " 1570.00044599  787.00002547 7337.9998861  7313.99965535 4206.99955709\n",
      " 2601.99974976 3419.00004147 5233.99991073 9436.99961808 9871.99924313\n",
      " 7454.99935308 2553.0002172   645.00016376 6049.99943143 7390.99937811\n",
      " 2525.99985462 9473.99965999 8962.99954246 3008.00011265 7389.99983581\n",
      "  465.0000358  2556.00011375 3161.99994894 3990.9994463  8863.99942413\n",
      " 4152.99993709 7975.99965022 6949.99963577 2635.99951898 2933.99977057\n",
      " 6885.99991714 7658.99956332 7603.00020793 5285.99978823 6706.99977976\n",
      " 2899.99989217 3433.99991583 3754.0000328  3603.000182   2873.99977553\n",
      " 6784.99982053 7396.99969192 9186.99973042 6598.99994235 1834.00063642\n",
      "  941.99986889 6433.99989203 5895.99965126 1820.99979723 8179.9997728\n",
      " 2026.99998536 5444.99974259 9722.99893245 2141.99996484  737.99977349\n",
      " 9723.99966542 4054.00032615 8877.99987936 5446.99958611 8697.99926018\n",
      " 3544.99984154 3991.99984057 1157.99998488 3199.99993396 6223.99969272\n",
      " 3983.99935284 9620.99997843  865.00025425 6264.99971183  658.00035923\n",
      " 6646.99948786 1983.00016911 3093.99947987 5037.99966931 2000.99934184\n",
      "  453.99997747 2878.00038384 1986.99978023  615.99982693 7870.99959807]\n",
      "[1430 3850 5648 4367 4363 1962 5762 6793  774 3129 6884 7927 2186 2802\n",
      " 5505 8225 3693 2870 8940 4301 1570  787 7338 7314 4207 2602 3419 5234\n",
      " 9437 9872 7455 2553  645 6050 7391 2526 9474 8963 3008 7390  465 2556\n",
      " 3162 3991 8864 4153 7976 6950 2636 2934 6886 7659 7603 5286 6707 2900\n",
      " 3434 3754 3603 2874 6785 7397 9187 6599 1834  942 6434 5896 1821 8180\n",
      " 2027 5445 9723 2142  738 9724 4054 8878 5447 8698 3545 3992 1158 3200\n",
      " 6224 3984 9621  865 6265  658 6647 1983 3094 5038 2001  454 2878 1987\n",
      "  616 7871]\n",
      "-0.016723997413748748\n"
     ]
    }
   ],
   "source": [
    "print(theta)\n",
    "print(a)\n",
    "print(sum(theta-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4aa46499b0>]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(hist_cost)),hist_cost,'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computers nowadays are really amazing, my computer done $17591 \\times 20000 \\times 100$ in 370 seconds, which is about 37.5 billion calculations to reach a cost less than 0.00001. It did nearly 95 million calculations in 1 second...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
